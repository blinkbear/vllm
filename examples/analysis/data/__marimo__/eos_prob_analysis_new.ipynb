{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import marimo as mo\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib as mpl\n",
    "import rich.console\n",
    "\n",
    "\n",
    "_orig_console = rich.console.Console\n",
    "\n",
    "\n",
    "class Console(_orig_console):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs[\"force_terminal\"] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "rich.console.Console = Console\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"] = 42\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=16)\n",
    "base_dir=\"/root/vllm/examples/analysis/data/eos_result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<span class=\"codehilite\"><div class=\"highlight\"><pre><span></span><span class=\"gt\">Traceback (most recent call last):</span>\n",
      "  File <span class=\"nb\">&quot;/usr/local/lib/python3.10/dist-packages/marimo/_runtime/executor.py&quot;</span>, line <span class=\"m\">141</span>, in <span class=\"n\">execute_cell</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">exec</span><span class=\"p\">(</span><span class=\"n\">cell</span><span class=\"o\">.</span><span class=\"n\">body</span><span class=\"p\">,</span> <span class=\"n\">glbls</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;/tmp/marimo_3923715/__marimo__cell_MJUe_.py&quot;</span>, line <span class=\"m\">9</span>, in <span class=\"n\">&lt;module&gt;</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">tmp_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span>\n",
      "  File <span class=\"nb\">&quot;/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py&quot;</span>, line <span class=\"m\">1026</span>, in <span class=\"n\">read_csv</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">return</span> <span class=\"n\">_read</span><span class=\"p\">(</span><span class=\"n\">filepath_or_buffer</span><span class=\"p\">,</span> <span class=\"n\">kwds</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py&quot;</span>, line <span class=\"m\">626</span>, in <span class=\"n\">_read</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">return</span> <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"n\">nrows</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py&quot;</span>, line <span class=\"m\">1923</span>, in <span class=\"n\">read</span>\n",
      "<span class=\"w\">    </span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_engine</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(</span>  <span class=\"c1\"># type: ignore[attr-defined]</span>\n",
      "  File <span class=\"nb\">&quot;/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py&quot;</span>, line <span class=\"m\">234</span>, in <span class=\"n\">read</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">chunks</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_reader</span><span class=\"o\">.</span><span class=\"n\">read_low_memory</span><span class=\"p\">(</span><span class=\"n\">nrows</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;parsers.pyx&quot;</span>, line <span class=\"m\">838</span>, in <span class=\"n\">pandas._libs.parsers.TextReader.read_low_memory</span>\n",
      "  File <span class=\"nb\">&quot;parsers.pyx&quot;</span>, line <span class=\"m\">905</span>, in <span class=\"n\">pandas._libs.parsers.TextReader._read_rows</span>\n",
      "  File <span class=\"nb\">&quot;parsers.pyx&quot;</span>, line <span class=\"m\">874</span>, in <span class=\"n\">pandas._libs.parsers.TextReader._tokenize_rows</span>\n",
      "  File <span class=\"nb\">&quot;parsers.pyx&quot;</span>, line <span class=\"m\">891</span>, in <span class=\"n\">pandas._libs.parsers.TextReader._check_tokenize_status</span>\n",
      "  File <span class=\"nb\">&quot;parsers.pyx&quot;</span>, line <span class=\"m\">2053</span>, in <span class=\"n\">pandas._libs.parsers.raise_parser_error</span>\n",
      "<span class=\"gr\">KeyboardInterrupt</span>\n",
      "</pre></div>\n",
      "</span>"
     ]
    }
   ],
   "source": [
    "model_names = [\"llama\", \"mistral\"]\n",
    "dataset_names = [\"alpaca\", \"sharegpt\"]\n",
    "dataset_name_map = {\"alpaca\": \"Alpaca\", \"sharegpt\": \"ShareGPT\"}\n",
    "model_name_map = {\"llama\": \"Llama\", \"mistral\": \"Mistral\"}\n",
    "\n",
    "eos_prob_rank_result_df = pd.DataFrame()\n",
    "for model_name in model_names:\n",
    "    for dataset_name in dataset_names:\n",
    "        tmp_df = pd.read_csv(\n",
    "            f\"{base_dir}/{model_name}_{dataset_name}_eos_prob_result.csv\"\n",
    "        )\n",
    "        tmp_df[\"model_dataset\"] = (\n",
    "            model_name_map[model_name] + \" \" + dataset_name_map[dataset_name]\n",
    "        )\n",
    "        eos_prob_rank_result_df = pd.concat([eos_prob_rank_result_df, tmp_df])\n",
    "eos_prob_rank_result_df = eos_prob_rank_result_df[\n",
    "    eos_prob_rank_result_df[\"eos_prob\"] != 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_rank(group):\n",
    "    # Find the row with the minimum token_num\n",
    "    min_token_num = group[\"token_num\"][:20].min()\n",
    "    min_token_row = group[group[\"token_num\"] == min_token_num]\n",
    "\n",
    "    # Find the maximum token_num\n",
    "    max_token_num = group[\"token_num\"].max()\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"min_eos_token_rank\": [min_token_row[\"eos_token_rank\"].values[0]],\n",
    "            \"max_token_num\": [max_token_num],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Assuming eos_prob_rank_result_df is your DataFrame\n",
    "result = (\n",
    "    eos_prob_rank_result_df.groupby([\"request_id\", \"model_dataset\"])\n",
    "    .parallel_apply(get_init_rank)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten the resulting DataFrame\n",
    "result = result.drop(columns=[\"level_2\"]).rename(columns={0: \"result\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_max_token_nums(row):\n",
    "    eos_prob = max(row[\"eos_prob\"][: min(len(row), 15)])\n",
    "    max_token_nums = max(row[\"token_num\"])\n",
    "    return eos_prob, max_token_nums\n",
    "\n",
    "\n",
    "eos_prob_result = (\n",
    "    eos_prob_rank_result_df[\n",
    "        (\n",
    "            eos_prob_rank_result_df[\"prompt_len\"]\n",
    "            + eos_prob_rank_result_df[\"token_num\"]\n",
    "            < 2048\n",
    "        )\n",
    "        & (eos_prob_rank_result_df[\"token_num\"] > 15)\n",
    "    ]\n",
    "    .groupby([\"request_id\", \"model_dataset\"])\n",
    "    .parallel_apply(\n",
    "        lambda row: pd.Series(\n",
    "            get_eos_max_token_nums(row), index=[\"eos_prob\", \"max_token_nums\"]\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_prob_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def get_predicted_result(row):\n",
    "    def linear_regression(X, y):\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        result = model.predict(X)\n",
    "        print(\"MSE: \", mean_squared_error(y, result))\n",
    "        print(\"MAE\", mean_absolute_error(y, result))\n",
    "        print(\"R-squared: \", model.score(X, y))\n",
    "\n",
    "    X = row[[\"eos_prob\"]]\n",
    "    y = row[\"max_token_nums\"]\n",
    "    linear_regression(X, y)\n",
    "\n",
    "\n",
    "eos_prob_result.groupby([\"model_dataset\"])[\n",
    "    [\"eos_prob\", \"max_token_nums\"]\n",
    "].apply(lambda row: get_predicted_result(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "\n",
    "def calc_eos_prob_output_len_corr(row):\n",
    "    init_length = 15\n",
    "    if (\n",
    "        len(row[\"eos_token_rank\"]) < init_length\n",
    "        or max(row[\"token_num\"]) < init_length\n",
    "    ):\n",
    "        return -1\n",
    "\n",
    "    token_nums = max(row[\"token_num\"]) - row[\"token_num\"]\n",
    "    token_nums = np.array(token_nums.tolist())\n",
    "    eos_token_probs = row[\"eos_prob\"].tolist()\n",
    "    max_eos_token_prob = np.array(\n",
    "        [\n",
    "            max(eos_token_probs[max(-init_length, 0) : i])\n",
    "            for i in range(1, len(eos_token_probs) + 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pearsonr(token_nums, max_eos_token_prob).statistic\n",
    "\n",
    "\n",
    "eos_prob_seq_len_corrs = (\n",
    "    eos_prob_rank_result_df.groupby([\"request_id\", \"model_dataset\"])\n",
    "    .parallel_apply(lambda row: calc_eos_prob_output_len_corr(row))\n",
    "    .reset_index()\n",
    ")\n",
    "eos_prob_seq_len_corrs[\"corr\"] = eos_prob_seq_len_corrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2.5), dpi=120)\n",
    "ax = sns.ecdfplot(\n",
    "    eos_prob_seq_len_corrs[eos_prob_seq_len_corrs[\"corr\"] > -1],\n",
    "    x=\"corr\",\n",
    "    hue=\"model_dataset\",\n",
    ")\n",
    "ax_legend = ax.get_legend()\n",
    "ax.xaxis.set_major_locator(MultipleLocator(0.2))\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.xlabel(\"Corr\", fontsize=12)\n",
    "plt.ylabel(\"Proportion\", fontsize=12)\n",
    "ax_legend.set(ncols=4, frame_on=False, title=\"\")\n",
    "plt.grid(alpha=0.3, linestyle=\"--\")\n",
    "plt.tight_layout(h_pad=0, pad=0.1)\n",
    "plt.savefig(\"/root/vllm/examples/analysis/data/fig/seq_len_eos_rank_corr.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_expected_left_length(row):\n",
    "    if len(row) > 2048 or max(row[\"token_num\"]) < init_length:\n",
    "        return -1, -1\n",
    "\n",
    "    token_nums = max(row[\"token_num\"])\n",
    "    eos_token_ranks = row[\"eos_prob\"].tolist()\n",
    "    differences = (\n",
    "        np.max(eos_token_ranks[:init_length])\n",
    "        - np.min(eos_token_ranks[:init_length])\n",
    "    ) / init_length\n",
    "    # average_difference = np.mean(differences)\n",
    "    # differences = np.diff(max_eos_token_rank[:5])\n",
    "    # differences = np.mean(differences)\n",
    "    # differences = np.std(eos_token_ranks[:init_length])\n",
    "    return differences, token_nums\n",
    "\n",
    "def get_estimate_accuracy(expected_accuracy, model):\n",
    "    llama_alpaca_expected_accuracy = expected_accuracy[\n",
    "        expected_accuracy[\"model_dataset\"] == model\n",
    "    ]\n",
    "    selected_llama_alpaca_expected_accuracy = (\n",
    "        llama_alpaca_expected_accuracy.sample(6000)\n",
    "    )\n",
    "    llama_alpaca_expected_accuracy_merge = (\n",
    "        selected_llama_alpaca_expected_accuracy.assign(key=1)\n",
    "        .merge(selected_llama_alpaca_expected_accuracy.assign(key=1), on=\"key\")\n",
    "        .drop(\"key\", axis=1)\n",
    "    )\n",
    "    llama_alpaca_expected_accuracy_merge[\"req_ids\"] = (\n",
    "        llama_alpaca_expected_accuracy_merge[\"request_id_x\"]\n",
    "        + llama_alpaca_expected_accuracy_merge[\"request_id_y\"]\n",
    "    )\n",
    "    llama_alpaca_expected_accuracy_merge = (\n",
    "        llama_alpaca_expected_accuracy_merge.drop_duplicates(subset=\"req_ids\")\n",
    "    )\n",
    "    filter1 = (\n",
    "        llama_alpaca_expected_accuracy_merge[\"eos_prob_diff_x\"]\n",
    "        >= llama_alpaca_expected_accuracy_merge[\"eos_prob_diff_y\"]\n",
    "    )\n",
    "    filter2 = (\n",
    "        llama_alpaca_expected_accuracy_merge[\"max_token_nums_x\"]\n",
    "        <= llama_alpaca_expected_accuracy_merge[\"max_token_nums_y\"]\n",
    "    )\n",
    "    filter3 = (\n",
    "        llama_alpaca_expected_accuracy_merge[\"eos_prob_diff_x\"]\n",
    "        <= llama_alpaca_expected_accuracy_merge[\"eos_prob_diff_y\"]\n",
    "    )\n",
    "    filter4 = (\n",
    "        llama_alpaca_expected_accuracy_merge[\"max_token_nums_x\"]\n",
    "        >= llama_alpaca_expected_accuracy_merge[\"max_token_nums_y\"]\n",
    "    )\n",
    "    accuracy = len(\n",
    "        llama_alpaca_expected_accuracy_merge[\n",
    "            (filter1 & filter2) | (filter3 & filter4)\n",
    "        ]\n",
    "    ) / len(llama_alpaca_expected_accuracy_merge)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_expected_accuracy_df(eos_prob_rank_result_df):\n",
    "    expected_accuracy = (\n",
    "        eos_prob_rank_result_df.groupby([\"request_id\", \"model_dataset\"])\n",
    "        .parallel_apply(\n",
    "            lambda row: pd.Series(\n",
    "                calc_expected_left_length(row),\n",
    "                index=[\"eos_prob_diff\", \"max_token_nums\"],\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    expected_accuracy = expected_accuracy[\n",
    "        expected_accuracy[\"eos_prob_diff\"] != -1\n",
    "    ]\n",
    "    return expected_accuracy\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"Llama Alpaca\",\n",
    "    \"Llama ShareGPT\",\n",
    "    \"Mistral Alpaca\",\n",
    "    \"Mistral ShareGPT\",\n",
    "]\n",
    "init_lengths = [10, 15, 20, 25]\n",
    "\n",
    "estimated_accuracy = {\"models\": [], \"init_length\": [], \"accuracy\": []}\n",
    "\n",
    "for init_length in init_lengths:\n",
    "    expected_accuracy = get_expected_accuracy_df(eos_prob_rank_result_df)\n",
    "    for model in models:\n",
    "        accuracy = get_estimate_accuracy(expected_accuracy, model)\n",
    "        estimated_accuracy[\"models\"].append(model)\n",
    "        estimated_accuracy[\"init_length\"].append(init_length)\n",
    "        estimated_accuracy[\"accuracy\"].append(accuracy)\n",
    "estimated_accuracy = pd.DataFrame(estimated_accuracy)\n",
    "estimated_accuracy.to_csv(f\"{base_dir}/estimated_accuracy_cov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2.5), dpi=120)\n",
    "sns.barplot(\n",
    "    data=estimated_accuracy,\n",
    "    x=\"init_length\",\n",
    "    y=\"accuracy\",\n",
    "    hue=\"models\",\n",
    "    zorder=10,\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(\n",
    "    loc=\"best\",\n",
    "    ncol=2,\n",
    "    frameon=False,\n",
    "    labelspacing=0.1,\n",
    "    handlelength=1,\n",
    "    handletextpad=0.3,\n",
    "    columnspacing=0.3,\n",
    ")\n",
    "plt.xlabel(\"Init Length\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.5, ls=\"--\", zorder=0)\n",
    "plt.savefig(\"estimated_accuracy.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "disabled": true
     }
    }
   },
   "source": [
    "## test warm up window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "disabled": true,
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def max_eos_prob_left_seq_len(row, i):\n",
    "    if len(row) <= i:\n",
    "        return pd.Series([-1, -1], index=[\"max_eos_prob\", \"left_seq_len\"])\n",
    "    # max_eos_prob = np.std(row[\"eos_prob\"][:i]) / np.mean(\n",
    "    #     row[\"eos_prob\"][:i]\n",
    "    # )\n",
    "    max_eos_prob = np.max(row[\"eos_token_rank\"][:i])\n",
    "    left_seq_len = max(row[\"token_num\"]) - min(row[\"token_num\"][i:])\n",
    "    tmp_df = pd.Series(\n",
    "        [max_eos_prob, left_seq_len], index=[\"max_eos_prob\", \"left_seq_len\"]\n",
    "    )\n",
    "\n",
    "\n",
    "corrs = []\n",
    "for i in range(1, 50):\n",
    "    eos_prob_left_seq_len = (\n",
    "        eos_prob_rank_result_df.groupby([\"request_id\"])\n",
    "        .parallel_apply(lambda row: max_eos_prob_left_seq_len(row, i))\n",
    "        .reset_index()\n",
    "        .drop(columns=[\"request_id\"])\n",
    "    )\n",
    "    corrs.append(\n",
    "        eos_prob_left_seq_len[eos_prob_left_seq_len[\"max_eos_prob\"] >= 0]\n",
    "        .corr(method=\"spearman\")\n",
    "        .loc[\"max_eos_prob\", \"left_seq_len\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2.5), dpi=150)\n",
    "print(corrs)\n",
    "sns.lineplot(corrs, hue=\"model_dataset\")\n",
    "plt.xlabel(\"# of Iters\")\n",
    "plt.ylabel(\"Corr. Seq Len vs. Prob\")\n",
    "plt.grid(alpha=0.3, linestyle=\"--\")\n",
    "plt.show()\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "disabled": true,
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def eos_prob_predict_len(row, i):\n",
    "    if len(row) <= i:\n",
    "        return pd.Series([-1, -1], index=[\"pred_len\", \"left_seq_len\"])\n",
    "    # max_eos_prob = np.std(row[\"eos_token_rank\"][:i]) / np.mean(\n",
    "    #     row[\"eos_prob\"][:i]\n",
    "    # )\n",
    "\n",
    "    max_eos_prob = 1 - np.max(row[\"eos_token_rank\"][:i]) / 32000\n",
    "    _n = int(-np.log(np.max(row[\"eos_token_rank\"][:i]))) + 23\n",
    "    predict_len = (\n",
    "        max_eos_prob\n",
    "        * (1 + _n * max_eos_prob ** (_n + 1) - (_n + 1) * max_eos_prob**_n)\n",
    "        / ((1 - max_eos_prob) ** 2)\n",
    "    )\n",
    "\n",
    "    left_seq_len = max(row[\"token_num\"]) - min(row[\"token_num\"][i:])\n",
    "    return pd.Series(\n",
    "        [predict_len, left_seq_len], index=[\"pred_len\", \"left_seq_len\"]\n",
    "    )\n",
    "\n",
    "\n",
    "eos_pred_len_df = (\n",
    "    eos_prob_rank_result_df.groupby([\"request_id\"])\n",
    "    .apply(\n",
    "        lambda row: eos_prob_predict_len(row, 3),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index()\n",
    "    .drop(columns=[\"request_id\"])\n",
    ")\n",
    "eos_pred_len_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "disabled": true,
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "_tmp_eos_prob_rank_result_df = eos_prob_rank_result_df[\n",
    "    eos_prob_rank_result_df[\"request_id\"] == 0\n",
    "]\n",
    "_tmp_eos_prob_rank_result_df[\"left_seq_len\"] = (\n",
    "    max(_tmp_eos_prob_rank_result_df[\"token_num\"])\n",
    "    - _tmp_eos_prob_rank_result_df[\"token_num\"]\n",
    ")\n",
    "_tmp_eos_prob_rank_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "disabled": true,
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "seq_output_len = (\n",
    "    eos_prob_rank_result_df.groupby([\"prompt_len\"])\n",
    "    .agg({\"token_num\": \"mean\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
